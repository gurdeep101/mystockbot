{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader, Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found\n"
     ]
    }
   ],
   "source": [
    "# check for llamaparse key\n",
    "LLAMAPARSE_API_KEY = os.environ.get('LLAMAPARSE_API_KEY')\n",
    "if LLAMAPARSE_API_KEY is not None:\n",
    "    print('API key found')\n",
    "else:\n",
    "    print('Check for API key in environment variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ufr-q1fy24.pdf', 'ufr-q3fy24.pdf', 'ufr-q2fy24.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of files over which to prep\n",
    "file_list = [file for file in os.listdir('../data') if file.startswith('ufr')]\n",
    "file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(3, 11), match='-q1fy24.'>\n",
      "<re.Match object; span=(4, 10), match='q1fy24'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'q1fy24'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive loobehind assertion and positive lookahead assertion in regex to get filename\n",
    "\n",
    "pattern = r\"(?<=-)\\w+(?=\\.)\"\n",
    "\n",
    "re_dash_to_dot = re.compile(r\"-([^\\.]+)\\.\")\n",
    "print(re.search(re_dash_to_dot, file_list[0]))\n",
    "print(re.search(pattern, file_list[0]))\n",
    "re.search(pattern, file_list[0]).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axis-q1fy24', 'axis-q3fy24', 'axis-q2fy24']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of variables over which to construct the indexes\n",
    "# positive loobehind assertion and positive lookahead assertion in regex to get filename\n",
    "pattern = r\"(?<=-)\\w+(?=\\.)\"\n",
    "doc_names = ['axis-'+re.search(pattern, file).group() for file in file_list]\n",
    "doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axis-q1fy24-index', 'axis-q3fy24-index', 'axis-q2fy24-index']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for separate index for each document\n",
    "# list of variables over which to construct the indexes\n",
    "# positive loobehind assertion and positive lookahead assertion in regex to get filename\n",
    "pattern = r\"(?<=-)\\w+(?=\\.)\"\n",
    "vec_index_list = ['axis-'+re.search(pattern, file).group()+'-index' for file in file_list]\n",
    "vec_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fy24', 'fy24', 'fy24']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"\\w{4}(?=\\.)\"\n",
    "year = [re.search(pattern, file).group() for file in file_list]\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q1', 'q3', 'q2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"(?<=-)\\w{2}\"\n",
    "quarter = [re.search(pattern, file).group() for file in file_list]\n",
    "quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate parser\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMAPARSE_API_KEY,\n",
    "    result_type=\"markdown\", # or text\n",
    "    num_workers=4, # for multiple files\n",
    "    verbose=True,\n",
    "    language=\"en\", # default is english\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename as metadata\n",
    "file_extractor = {\".pdf\": parser}\n",
    "filename_fn = lambda filename: {\"file_name\": filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ufr-q1fy24.pdf axis-q1fy24 q1 fy24\n",
      "ufr-q3fy24.pdf axis-q3fy24 q3 fy24\n",
      "ufr-q2fy24.pdf axis-q2fy24 q2 fy24\n"
     ]
    }
   ],
   "source": [
    "for file, doc_name, qtr, yr in zip(file_list, doc_names, quarter, year):\n",
    "    print(file, doc_name, qtr, yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 33b8b6ab-ad0e-40c6-b694-193d234ef428\n",
      "Started parsing the file under job_id 912f88ab-12f0-48bf-b386-5858e266a739\n",
      "Started parsing the file under job_id 461a237d-42b9-4096-a34f-69bace366987\n"
     ]
    }
   ],
   "source": [
    "# read in docs\n",
    "doc_dict = {}\n",
    "\n",
    "for file, doc_name, qtr, yr in zip(file_list, doc_names, quarter, year):\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_files=['../data/' + file],\n",
    "        file_extractor=file_extractor,\n",
    "        # filename_as_id=True,\n",
    "        file_metadata=filename_fn,\n",
    "        )\n",
    "    doc = reader.load_data()\n",
    "    for i in doc:\n",
    "        i.metadata={\n",
    "            'file_descr':f'Axis bank quarterly earnings report for {re.search(pattern, file).group()} ',\n",
    "            'financial_year': yr,\n",
    "            'quarter':qtr,\n",
    "                }\n",
    "        # i.metadata['file_descr']=str(f'Axis bank quarterly earnings report for {re.search(pattern, file).group()} '),\n",
    "        # i.metadata['financial year']=str(yr),\n",
    "        # i.metadata['quarter']=str(qtr),\n",
    "    doc_dict[doc_name] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_dict['axis-q1fy24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(variable, expected_length):\n",
    "    try:\n",
    "        if len(variable)==len(expected_length):\n",
    "            print('All files parsed')\n",
    "        else:\n",
    "            raise ValueError(f'Number of files parse is not equal to {len(expected_length)}')\n",
    "    except ValueError as e:\n",
    "        print(f'Caught Exception: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files parsed\n"
     ]
    }
   ],
   "source": [
    "check_length(doc_dict, file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more imports\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate llm and markdown parser\n",
    "llm = OpenAI(model='gpt-3.5-turbo-0125', temperature=0.1)\n",
    "node_parser=MarkdownElementNodeParser(llm=llm, num_workers=4)\n",
    "storage_context = StorageContext.from_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(f'../data/axis_qr_index')\n",
    "\n",
    "if not data_path.exists():\n",
    "    Path.mkdir(data_path)\n",
    "    print(data_path)\n",
    "    print('data path created')\n",
    "else:\n",
    "    print(data_path)\n",
    "    print('data path exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the doc_list dictionary to parse the documents, construct vector index and store to disk\n",
    "for doc in doc_dict.keys():\n",
    "    # print(doc)\n",
    "    documents = doc_dict[doc]\n",
    "    # run parser and get nodes for text and summary for tables\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    base_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
    "    index = VectorStoreIndex(nodes=base_nodes+objects, storage_context=storage_context)\n",
    "    index.set_index_id(doc)\n",
    "    index.storage_context.persist(persist_dir=f'../data/axis_qr_index')\n",
    "    print(f'{doc} indexed and stored successfully to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stored index \n",
    "local_index= {}\n",
    "for name in doc_names:   \n",
    "    storage_context=StorageContext.from_defaults(\n",
    "        persist_dir=f'../data/axis_qr_index')\n",
    "    cur_index=load_index_from_storage(storage_context, index_id=name)\n",
    "    local_index[name]=cur_index\n",
    "    print(f'loaded index {name} from local storage')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://docs.llamaindex.ai/en/stable/module_guides/storing/customization/\n",
    "# # loop over the variable list and document dictionary to parse the nodes and construct indexes\n",
    "# for file_list_item, vec_index_list_item in zip(file_list, vec_index_list):\n",
    "    \n",
    "#     # store vector index to disk\n",
    "#     data_path = Path(f'../data/axis_qr_index/{vec_index_list_item}')\n",
    "#     # print(data_path)\n",
    "#     if not data_path.exists():\n",
    "#         Path.mkdir(data_path, parents=True, exist_ok=True)\n",
    "#         # print(data_path)\n",
    "    \n",
    "#     documents = doc_list[file_list_item]\n",
    "#     # run parser and get nodes for text and summary for tables\n",
    "#     nodes = node_parser.get_nodes_from_documents(documents)\n",
    "#     base_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
    "#     index = VectorStoreIndex(nodes=base_nodes+objects, storage_context=storage_context)\n",
    "    \n",
    "#     index.storage_context.persist(persist_dir=data_path)\n",
    "#     print(f'{file_list_item} indexed and stored successfully to disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing on Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gurdeep/miniconda3/envs/streamchat/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# vector store imports\n",
    "# !pip install llama-index-vector-stores-pinecone\n",
    "# !pip install pinecone-client\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pc = Pinecone()\n",
    "\n",
    "# dictionary of indexes\n",
    "pc_index_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexes': []}\n",
      "Index 'axis-q1fy24-index' does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all existing indexes\n",
    "indexes = pc.list_indexes()\n",
    "print(indexes)\n",
    "\n",
    "index_names = [i['name'] for i in pc.list_indexes()]\n",
    "\n",
    "# Check if a specific index exists\n",
    "index_name = \"axis-q1fy24-index\"\n",
    "if index_name not in index_names:\n",
    "    print(f\"Index '{index_name}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all existing indexes\n",
    "# indexes = pc.list_indexes()\n",
    "# pc.list_indexes().names()# \n",
    "# index_names = [i['name'] for i in pc.list_indexes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis-q1fy24-index created successfully\n",
      "{'dimension': 1536,\n",
      " 'host': 'axis-q1fy24-index-a0ad14b.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'axis-q1fy24-index',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n",
      "axis-q3fy24-index created successfully\n",
      "{'dimension': 1536,\n",
      " 'host': 'axis-q3fy24-index-a0ad14b.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'axis-q3fy24-index',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n",
      "axis-q2fy24-index created successfully\n",
      "{'dimension': 1536,\n",
      " 'host': 'axis-q2fy24-index-a0ad14b.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'axis-q2fy24-index',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n"
     ]
    }
   ],
   "source": [
    "index_names = [i['name'] for i in pc.list_indexes()]\n",
    "\n",
    "# create separate serverless pinecone index for each index in index_set; i.e. quarterly result\n",
    "for item in vec_index_list:\n",
    "    # print(item)\n",
    "    if item not in index_names:\n",
    "        # print(item)\n",
    "        # create index\n",
    "        pc.create_index(\n",
    "            name=item,\n",
    "            dimension=1536,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\",\n",
    "            )\n",
    "        )\n",
    "        pc_index_dict[item] = pc.Index(item)\n",
    "        print(f'{item} created successfully')\n",
    "        print(pc.describe_index(name=item))\n",
    "    else:\n",
    "        pc_index_dict[item] = pc.Index(item)\n",
    "        print(f'{item} exists and is loaded successfully')\n",
    "        print(pc.describe_index(name=item))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'fin_index' not in pc.list_indexes():\n",
    "#     # create index\n",
    "#     pc.create_index(\n",
    "#         name=\"fin-index\",\n",
    "#         dimension=1536,\n",
    "#         metric=\"cosine\",\n",
    "#         spec=ServerlessSpec(\n",
    "#             cloud=\"aws\",\n",
    "#             region=\"us-east-1\",\n",
    "#         )\n",
    "#     )\n",
    "#     print('finIndex created successfully')\n",
    "#     print(pc.describe_index(name=\"fin-index\"))\n",
    "#     # initialize index\n",
    "#     fin_index = pc.Index('fin-index')\n",
    "# else:\n",
    "#     fin_index = pc.Index('fin-index')\n",
    "#     print('finIndex loaded successfully')\n",
    "#     print(pc.describe_index(name=\"fin-index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['axis-q1fy24', 'axis-q3fy24', 'axis-q2fy24'],\n",
       " dict_keys(['axis-q1fy24', 'axis-q3fy24', 'axis-q2fy24']))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_names, doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught exception: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'x-pinecone-api-version': '2024-04', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'X-Cloud-Trace-Context': '1d46e995f7565f84d62cee92aa5fc303', 'Date': 'Fri, 17 May 2024 08:02:30 GMT', 'Server': 'Google Frontend', 'Content-Length': '79', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"error\":{\"code\":\"NOT_FOUND\",\"message\":\"Resource test not found\"},\"status\":404}\n",
      "\n",
      "index not found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # delete index \n",
    "    pc.delete_index(\"test\")\n",
    "except Exception as e:\n",
    "    print(f'caught exception: {e}')\n",
    "    print('index not found')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index \"test\" exists; loaded and initialised successfully as test_index\n",
      "{'dimension': 1536,\n",
      " 'host': 'test-a0ad14b.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'test',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n"
     ]
    }
   ],
   "source": [
    "new_index = 'test'\n",
    "index_names = [i['name'] for i in pc.list_indexes()]\n",
    "\n",
    "# create single serverless pinecone index for each index in index_set\n",
    "if new_index not in index_names:\n",
    "    # print(item)\n",
    "    # create index\n",
    "    pc.create_index(\n",
    "        name=new_index,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    )\n",
    "    test_index = pc.Index(new_index)\n",
    "    # pc_index_dict[new_index] = pc.Index(new_index)\n",
    "    print(f'Index \"{new_index}\" created and initialised successfully')\n",
    "    print(pc.describe_index(name=new_index))\n",
    "else:\n",
    "    # pc_index_dict[new_index] = pc.Index(new_index)\n",
    "    test_index = pc.Index(new_index)\n",
    "    print(f'Index \"{new_index}\" exists; loaded and initialised successfully as {new_index}_index')\n",
    "    print(pc.describe_index(name=new_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 2530.86it/s]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.11s/it]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:01<00:00, 14.50it/s]\n",
      "Upserted vectors: 100%|██████████| 25/25 [00:03<00:00,  7.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize index\n",
    "# temp_index = pc.Index('test')\n",
    "\n",
    "# initialize pinecone vector store\n",
    "# metadata_filters = {\n",
    "#         'quarter': 'q1',\n",
    "#         'fiscal': 'fy24',\n",
    "#     }\n",
    "\n",
    "documents = doc_dict['axis-q1fy24']\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
    "\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=test_index,\n",
    "    namespace = 'axis bank')\n",
    "    # metadata_filters = metadata_filters)\n",
    "# create storage context with pinecone vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# create vector store index with documents and storage context\n",
    "item = VectorStoreIndex(\n",
    "    nodes=base_nodes+objects,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index \"axis-bank\" created and initialised successfully\n",
      "{'dimension': 1536,\n",
      " 'host': 'axis-bank-a0ad14b.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'axis-bank',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use namespaces and metadata in a single index for a bank\n",
    "\n",
    "index_names = [i['name'] for i in pc.list_indexes()]\n",
    "\n",
    "new_index = 'axis-bank'\n",
    "# create single serverless pinecone index for each index in index_set\n",
    "if new_index not in index_names:\n",
    "    # print(item)\n",
    "    # create index\n",
    "    pc.create_index(\n",
    "        name=new_index,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    )\n",
    "    axis_index = pc.Index(new_index)\n",
    "    # pc_index_dict[new_index] = pc.Index(new_index)\n",
    "    print(f'Index \"{new_index}\" created and initialised successfully')\n",
    "    print(pc.describe_index(name=new_index))\n",
    "else:\n",
    "    # pc_index_dict[new_index] = pc.Index(new_index)\n",
    "    axis_index = pc.Index(new_index)\n",
    "    print(f'Index \"{new_index}\" exists; loaded and initialised successfully')\n",
    "    print(pc.describe_index(name=new_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axis-q1fy24\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 6157.23it/s]\n",
      "100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:01<00:00, 17.39it/s]\n",
      "Upserted vectors: 100%|██████████| 25/25 [00:03<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"axis-q1fy24\" upserted into a separate namespace\n",
      "axis-q3fy24\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 17342.07it/s]\n",
      "100%|██████████| 21/21 [00:18<00:00,  1.13it/s]\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:01<00:00, 17.82it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreIndex' object has no attribute 'upsert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m PineconeVectorStore(\n\u001b[1;32m      8\u001b[0m     pinecone_index\u001b[38;5;241m=\u001b[39maxis_index,\n\u001b[1;32m      9\u001b[0m     namespace\u001b[38;5;241m=\u001b[39mdoc)\n\u001b[1;32m     10\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m---> 11\u001b[0m axis_index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_nodes\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m upserted into a separate namespace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/core/indices/vector_store/base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    302\u001b[0m ):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/core/indices/vector_store/base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/core/indices/vector_store/base.py:234\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m    233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[0;32m--> 234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node, new_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nodes_batch, new_ids):\n\u001b[1;32m    240\u001b[0m             \u001b[38;5;66;03m# NOTE: remove embedding from node to avoid duplication\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/streamchat/lib/python3.12/site-packages/llama_index/vector_stores/pinecone/base.py:393\u001b[0m, in \u001b[0;36mPineconeVectorStore.add\u001b[0;34m(self, nodes, **add_kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(node_id)\n\u001b[1;32m    392\u001b[0m     entries\u001b[38;5;241m.\u001b[39mappend(entry)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pinecone_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m(\n\u001b[1;32m    394\u001b[0m     entries,\n\u001b[1;32m    395\u001b[0m     namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace,\n\u001b[1;32m    396\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_kwargs,\n\u001b[1;32m    398\u001b[0m )\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VectorStoreIndex' object has no attribute 'upsert'"
     ]
    }
   ],
   "source": [
    "for doc in doc_names:\n",
    "    print(doc)\n",
    "    documents = doc_dict[doc]\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    base_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
    "    \n",
    "    vector_store = PineconeVectorStore(\n",
    "        pinecone_index=axis_index,\n",
    "        namespace=doc)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    axis_index = VectorStoreIndex(nodes=base_nodes+objects,\n",
    "                            storage_context=storage_context,\n",
    "                            show_progress=True)\n",
    "    print(f'\"{doc}\" upserted into a separate namespace')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query engine with reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from llama_index.postprocessor.flag_embedding_reranker import (\n",
    "    FlagEmbeddingReranker,\n",
    "    )\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize reranker\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=15,\n",
    "    model=\"BAAI/bge-reranker-large\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(doc_names))\n",
    "doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query engine tools with namespaces\n",
    "query_engine_tools =  [\n",
    "    QueryEngineTool(\n",
    "        query_engine=axis_index.as_query_engine(\n",
    "            similarity_top_k=15,\n",
    "            node_postprocessors=[reranker]),\n",
    "        metadata=ToolMetadata(\n",
    "            name=doc,\n",
    "            description=f'useful for when you want to answer queries about Axis bank quarterly earnings report for {re.search(pattern, file).group()}',\n",
    "            namespace=doc,\n",
    "        )\n",
    "    )\n",
    "    for doc in doc_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subquestion query engine\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    similarity_top_k=15,\n",
    "    use_async=True,\n",
    "    llm=llm,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
